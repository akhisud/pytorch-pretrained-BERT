{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREP FOR RETRIEVE AND EDIT USING OPENAI-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from pytorch_pretrained_bert import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, OpenAIAdam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import edit_distance\n",
    "from random import shuffle\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '/home/ubuntu/data/amazon'\n",
    "# DATA = '/home/ubuntu/data/yelp'\n",
    "\n",
    "VOCAB_PATH = os.path.join(DATA, 'vocab')\n",
    "ATTR_VOCAB_PATH = os.path.join(DATA, 'attribute_vocab')\n",
    "\n",
    "POS_TRAIN_FILE_PATH = os.path.join(DATA, 'sentiment.train.1')\n",
    "NEG_TRAIN_FILE_PATH = os.path.join(DATA,'sentiment.train.0')\n",
    "POS_TRAIN_OUT_FILE_PATH = POS_TRAIN_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "NEG_TRAIN_OUT_FILE_PATH = NEG_TRAIN_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "COMBINED_TRAIN_OUT_FILE_PATH = os.path.join(DATA, 'processed.sentiment.train.0.and.1')\n",
    "POS_REPLACE_ATTRS_PATH = os.path.join(DATA, 'attrs.replace.train.1')\n",
    "NEG_REPLACE_ATTRS_PATH = os.path.join(DATA, 'attrs.replace.train.0')\n",
    "\n",
    "POS_VAL_FILE_PATH = os.path.join(DATA, 'sentiment.dev.1')\n",
    "NEG_VAL_FILE_PATH = os.path.join(DATA,'sentiment.dev.0')\n",
    "POS_VAL_OUT_FILE_PATH = POS_VAL_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "NEG_VAL_OUT_FILE_PATH = NEG_VAL_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "COMBINED_VAL_OUT_FILE_PATH = os.path.join(DATA, 'processed.sentiment.dev.0.and.1')\n",
    "\n",
    "POS_TEST_FILE_PATH = os.path.join(DATA, 'sentiment.test.1')\n",
    "NEG_TEST_FILE_PATH = os.path.join(DATA,'sentiment.test.0')\n",
    "POS_TEST_OUT_FILE_PATH = POS_TEST_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "NEG_TEST_OUT_FILE_PATH = NEG_TEST_FILE_PATH.replace('sentiment', 'processed.sentiment')\n",
    "POS_TEST_OUT_FILE_INPUTS_PATH = os.path.join(DATA, 'processed.sentiment.test.inputs.1')\n",
    "NEG_TEST_OUT_FILE_INPUTS_PATH = os.path.join(DATA, 'processed.sentiment.test.inputs.0')\n",
    "COMBINED_TEST_OUT_FILE_PATH = os.path.join(DATA, 'processed.sentiment.test.0.and.1')\n",
    "COMBINED_TEST_OUT_FILE_INPUTS_PATH = os.path.join(DATA, 'processed.sentiment.test.inputs.0.and.1')\n",
    "\n",
    "POS_REF_FILE_PATH = os.path.join(DATA, 'reference.1')\n",
    "NEG_REF_FILE_PATH = os.path.join(DATA,'reference.0')\n",
    "FROM_POS_REF_FILE_PATH = os.path.join(DATA, 'reference.from.1')\n",
    "TO_NEG_REF_FILE_PATH = os.path.join(DATA,'reference.to.0')\n",
    "FROM_NEG_REF_FILE_PATH = os.path.join(DATA, 'reference.from.0')\n",
    "TO_POS_REF_FILE_PATH = os.path.join(DATA,'reference.to.1')\n",
    "FROM_POS_REF_OUT_FILE_PATH = os.path.join(DATA, 'processed.reference.from.1')\n",
    "FROM_NEG_REF_OUT_FILE_PATH = os.path.join(DATA, 'processed.reference.from.0')\n",
    "FROM_POS_REF_OUT_FILE_INPUTS_PATH = os.path.join(DATA, 'processed.reference.inputs.from.1')\n",
    "FROM_NEG_REF_OUT_FILE_INPUTS_PATH = os.path.join(DATA, 'processed.reference.inputs.from.0')\n",
    "\n",
    "POS_TRAIN_ATTRS_PATH = os.path.join(DATA, 'attrs.train.1')\n",
    "NEG_TRAIN_ATTRS_PATH = os.path.join(DATA, 'attrs.train.0')\n",
    "POS_TRAIN_CONTS_PATH = os.path.join(DATA, 'conts.train.1')\n",
    "NEG_TRAIN_CONTS_PATH = os.path.join(DATA, 'conts.train.0')\n",
    "\n",
    "FROM_POS_REF_ATTRS_PATH = os.path.join(DATA, 'attrs.ref.from.1')\n",
    "FROM_NEG_REF_ATTRS_PATH = os.path.join(DATA, 'attrs.ref.from.0')\n",
    "FROM_POS_REF_CONTS_PATH = os.path.join(DATA, 'conts.ref.from.1')\n",
    "FROM_NEG_REF_CONTS_PATH = os.path.join(DATA, 'conts.ref.from.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_NEG_TRAIN_FILE_PATH = os.path.join(DATA, 'sentiment.train.0.and.1')\n",
    "TMP = os.path.join(DATA, 'tmp')\n",
    "# SALIENCE = 5.5\n",
    "SALIENCE = 15\n",
    "LAMBDA = 1\n",
    "!cat $POS_TRAIN_FILE_PATH $NEG_TRAIN_FILE_PATH > $POS_NEG_TRAIN_FILE_PATH\n",
    "!python ./make_vcb.py $POS_NEG_TRAIN_FILE_PATH 40000 > $VOCAB_PATH\n",
    "!sed '1,4d' $VOCAB_PATH > $TMP\n",
    "!mv $TMP $VOCAB_PATH\n",
    "!python ./make_attr_vcb.py $VOCAB_PATH $POS_TRAIN_FILE_PATH $NEG_TRAIN_FILE_PATH $SALIENCE $LAMBDA > $ATTR_VOCAB_PATH\n",
    "                                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_reference_file(combined, left, right):\n",
    "    f = open(combined, 'r')\n",
    "    ref_combined = f.readlines()\n",
    "    ref_left = [s.strip().split('\\t')[0] for s in ref_combined]\n",
    "    ref_right = [s.strip().split('\\t')[1] for s in ref_combined]\n",
    "    f.close()\n",
    "    f = open(left, 'w')\n",
    "    f.write('\\n'.join(ref_left))\n",
    "    f.close()\n",
    "    f = open(right, 'w')\n",
    "    f.write('\\n'.join(ref_right))\n",
    "    f.close()  \n",
    "split_reference_file(POS_REF_FILE_PATH, FROM_POS_REF_FILE_PATH, TO_NEG_REF_FILE_PATH)\n",
    "split_reference_file(NEG_REF_FILE_PATH, FROM_NEG_REF_FILE_PATH, TO_POS_REF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': '<POS>',\n",
       " 'neg': '<NEG>',\n",
       " 'cont_start': '<CONT_START>',\n",
       " 'attr_start': '<ATTR_START>',\n",
       " 'data_start': '<DATA_START>',\n",
       " 'end': '<END>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_tokens = {'pos': '<POS>',\n",
    "              'neg': '<NEG>',\n",
    "              'cont_start':'<CONT_START>',\n",
    "              'attr_start':'<ATTR_START>',\n",
    "              'data_start':'<DATA_START>',\n",
    "              'end': '<END>'}\n",
    "spl_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_vocab = []\n",
    "with open(ATTR_VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        attribute_vocab.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attribute(attribute_vocab, line):\n",
    "    content = []\n",
    "    attribute = []\n",
    "    for token in line:\n",
    "        if token in attribute_vocab:\n",
    "            attribute.append(token)\n",
    "        else:\n",
    "            content.append(token)\n",
    "    return content, attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(reference_file_path):\n",
    "    attrs = []\n",
    "    conts = []\n",
    "    count = 0\n",
    "    outs = []\n",
    "    with open(reference_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            line = line.strip()\n",
    "            tokens = line.split(' ')\n",
    "            cont, attr = extract_attribute(attribute_vocab, tokens)\n",
    "            conts.append(' '.join(cont))\n",
    "            attrs.append(' '.join(attr))\n",
    "            if (count % 10000 == 0):\n",
    "                print(count)\n",
    "    return conts, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n"
     ]
    }
   ],
   "source": [
    "conts_pos, attrs_pos = extract_data(POS_TRAIN_FILE_PATH)\n",
    "conts_neg, attrs_neg = extract_data(NEG_TRAIN_FILE_PATH)\n",
    "conts_from_pos_ref, attrs_from_pos_ref = extract_data(FROM_POS_REF_FILE_PATH)\n",
    "conts_from_neg_ref, attrs_from_neg_ref = extract_data(FROM_NEG_REF_FILE_PATH)\n",
    "\n",
    "def write_list_to_file(l, file):\n",
    "    f = open(file, 'w')\n",
    "    for i in l:\n",
    "        f.write(i+'\\n')\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "write_list_to_file(conts_pos, POS_TRAIN_CONTS_PATH)\n",
    "write_list_to_file(conts_neg, NEG_TRAIN_CONTS_PATH)\n",
    "write_list_to_file(attrs_pos, POS_TRAIN_ATTRS_PATH)\n",
    "write_list_to_file(attrs_neg, NEG_TRAIN_ATTRS_PATH)\n",
    "\n",
    "write_list_to_file(conts_from_pos_ref, FROM_POS_REF_CONTS_PATH)\n",
    "write_list_to_file(conts_from_neg_ref, FROM_NEG_REF_CONTS_PATH)\n",
    "write_list_to_file(attrs_from_pos_ref, FROM_POS_REF_ATTRS_PATH)\n",
    "write_list_to_file(attrs_from_neg_ref, FROM_NEG_REF_ATTRS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(file):\n",
    "    f = open(file, 'r')\n",
    "    l = [line.strip() for line in f]\n",
    "    return l\n",
    "\n",
    "attrs_pos = read_list_from_file(POS_TRAIN_ATTRS_PATH)\n",
    "attrs_neg = read_list_from_file(NEG_TRAIN_ATTRS_PATH)\n",
    "conts_pos = read_list_from_file(POS_TRAIN_CONTS_PATH)\n",
    "conts_neg = read_list_from_file(NEG_TRAIN_CONTS_PATH)\n",
    "data_pos = read_list_from_file(POS_TRAIN_FILE_PATH)\n",
    "data_neg = read_list_from_file(NEG_TRAIN_FILE_PATH)\n",
    "\n",
    "conts_from_pos_ref = read_list_from_file(FROM_POS_REF_CONTS_PATH)\n",
    "conts_from_neg_ref = read_list_from_file(FROM_NEG_REF_CONTS_PATH)\n",
    "attrs_from_pos_ref = read_list_from_file(FROM_POS_REF_ATTRS_PATH)\n",
    "attrs_from_neg_ref = read_list_from_file(FROM_NEG_REF_ATTRS_PATH)\n",
    "data_from_pos_ref = read_list_from_file(FROM_POS_REF_FILE_PATH)\n",
    "data_from_neg_ref = read_list_from_file(FROM_NEG_REF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "conts_vecs = tfidf.fit_transform(conts_pos+conts_neg)\n",
    "conts_pos_vecs = conts_vecs[:len(conts_pos)]\n",
    "conts_neg_vecs = conts_vecs[len(conts_pos):len(conts_pos)+len(conts_neg)]\n",
    "conts_from_pos_ref_vecs = tfidf.transform(conts_from_pos_ref)\n",
    "conts_from_neg_ref_vecs = tfidf.transform(conts_from_neg_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.932584762573242\n"
     ]
    }
   ],
   "source": [
    "def calc_closest_content(l, conts_ref_vecs, conts_vecs, device='cpu'):\n",
    "    idxs = None\n",
    "    if l == conts_vecs.shape[0]:\n",
    "        idxs = np.array(range(l))\n",
    "    else:\n",
    "        idxs = np.random.choice(conts_vecs.shape[0], size=l, replace=False)\n",
    "    conts_rand_vecs = conts_vecs[idxs, :]\n",
    "    conts_rand_tensors = torch.tensor(conts_rand_vecs.todense())\n",
    "    conts_ref_tensors = torch.tensor(conts_ref_vecs.todense())\n",
    "    if device == 'gpu':\n",
    "        conts_rand_tensors = conts_rand_tensors.cuda()\n",
    "        conts_ref_tensors = conts_ref_tensors.cuda()\n",
    "    scores = torch.mm(conts_rand_tensors, conts_ref_tensors.transpose(0,1)).squeeze()\n",
    "    scores = scores.data.cpu().numpy()\n",
    "    ind_maxs = None\n",
    "    if len(scores.shape)>1:\n",
    "        ind_maxs = np.argmax(scores, axis=0)\n",
    "    else:\n",
    "        ind_maxs = np.array([np.argmax(scores)]) \n",
    "    closest = [idxs[ind] for ind in ind_maxs]\n",
    "    return closest\n",
    "\n",
    "s = time.time()\n",
    "attrs_tgt_from_pos_ref = calc_closest_content(50000, conts_from_pos_ref_vecs, conts_neg_vecs)\n",
    "attrs_tgt_from_neg_ref = calc_closest_content(50000, conts_from_neg_ref_vecs, conts_pos_vecs)\n",
    "print(time.time()-s)\n",
    "neg_tgt_attrs_ref_from_pos = [attrs_neg[i] for i in attrs_tgt_from_pos_ref]\n",
    "pos_tgt_attrs_ref_from_neg = [attrs_pos[i] for i in attrs_tgt_from_neg_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_replacements():\n",
    "    print(\"FROM POSITIVE:\")\n",
    "    for i in range(len(attrs_tgt_from_pos_ref)):\n",
    "        print(conts_from_pos_ref[i])\n",
    "        print(attrs_from_pos_ref[i])\n",
    "        print(conts_neg[attrs_tgt_from_pos_ref[i]])\n",
    "        print(attrs_neg[attrs_tgt_from_pos_ref[i]])\n",
    "        print('==========')\n",
    "        \n",
    "    print(\"FROM NEGATIVE:\")\n",
    "    for i in range(len(attrs_tgt_from_neg_ref)):\n",
    "        print(conts_from_neg_ref[i])\n",
    "        print(attrs_from_neg_ref[i])\n",
    "        print(conts_pos[attrs_tgt_from_neg_ref[i]])\n",
    "        print(attrs_pos[attrs_tgt_from_neg_ref[i]])\n",
    "        print('==========')\n",
    "# visualize_replacements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "def prepare_ref_data_file(conts_ref, tgt_attrs_ref, data, output_file_path):\n",
    "    count = 0\n",
    "    outs = []\n",
    "    for i in range(len(conts_ref)):\n",
    "        count += 1\n",
    "        ipstr = spl_tokens['attr_start'] + ' ' +  tgt_attrs_ref[i] + ' ' + spl_tokens['cont_start'] + ' ' + conts_ref[i]  + ' ' + spl_tokens[\n",
    "            'data_start'] + ' ' + data[i] + ' '+ spl_tokens['end'] + \"\\n\"\n",
    "        ipstr = ipstr.replace('  ', ' ')\n",
    "        outs.append(ipstr)\n",
    "        if (count % 100 == 0):\n",
    "            print(count)\n",
    "    out_file = open(output_file_path, 'w', encoding='utf-8')\n",
    "    for i in outs:\n",
    "        out_file.write(i)\n",
    "    out_file.close()\n",
    "prepare_ref_data_file(conts_from_pos_ref, neg_tgt_attrs_ref_from_pos, data_from_pos_ref, FROM_POS_REF_OUT_FILE_PATH)\n",
    "prepare_ref_data_file(conts_from_neg_ref, pos_tgt_attrs_ref_from_neg, data_from_neg_ref, FROM_NEG_REF_OUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_closest_attr(i):\n",
    "    idxs = np.random.randint(0, attrs_pos_vecs.shape[0], size=10000)\n",
    "    attrs_pos_rand_vecs = attrs_pos_vecs[idxs, :]\n",
    "    attrs_pos_rand_tensors = torch.tensor(attrs_pos_rand_vecs.todense()).cuda()\n",
    "    attrs_pos_curr_vec = attrs_pos_vecs[i]\n",
    "    attrs_pos_curr_tensor = torch.tensor(attrs_pos_curr_vec.todense()).cuda()\n",
    "    scores = torch.mm(attrs_pos_rand_tensors, attrs_pos_curr_tensor.transpose(0,1)).squeeze().data.cpu().numpy()\n",
    "    return np.argsort(scores)[::-1], [scores[k] for k in np.argsort(scores)[::-1]]\n",
    "    \n",
    "# for ind, _ in enumerate(attrs_pos_vecs):\n",
    "#     if attrs_pos[ind]:\n",
    "#         ret, scores = calc_closest_attr(ind)\n",
    "#         print(conts_pos[ind])\n",
    "#         print(attrs_pos[ind])\n",
    "#         print(ret.shape)\n",
    "#         print('=========')\n",
    "#         for i, sc in zip(ret[:100],scores[:100]):\n",
    "#             if attrs_pos[i]:\n",
    "#                 print(sc)\n",
    "#                 print(conts_pos[i])\n",
    "#                 print(repr(attrs_pos[i]))\n",
    "#                 print('===')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_closest_attr_edit_distance(attr, attrs_all):\n",
    "    idxs = np.random.randint(0, len(attrs_all), size=10000)\n",
    "    ret = None\n",
    "    for ind in idxs:\n",
    "        a_r = attrs_all[ind]\n",
    "        if edit_distance.SequenceMatcher(a=a_r.split(), b=attr.split()).distance()==1:\n",
    "            return ind\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data_file(conts, attrs, data, output_file_path, attr_replacements_file, sample_rate):\n",
    "    outs = []\n",
    "    repls = []\n",
    "    c_empty_attr = 0\n",
    "    c_noisy = 0\n",
    "    count = 0\n",
    "    for i, (cont, attr) in enumerate(zip(conts, attrs)):\n",
    "        count += 1\n",
    "        attr_noisy = attr\n",
    "        noisy_ind = None\n",
    "        if not attr:\n",
    "            c_empty_attr+=1\n",
    "        else:\n",
    "            noisy_ind = calc_closest_attr_edit_distance(attr, attrs)\n",
    "        if noisy_ind:\n",
    "            if random.random() < sample_rate:\n",
    "                attr_noisy = attrs[noisy_ind]\n",
    "                c_noisy+=1\n",
    "                replstr = (attr + '\\t' + attr_noisy + '\\n')\n",
    "                repls.append(replstr)\n",
    "        ipstr = spl_tokens['attr_start'] + ' ' + attr_noisy + ' ' + spl_tokens['cont_start'] + ' ' + cont  + \\\n",
    "            ' ' + spl_tokens['data_start'] + ' ' + data[i] + ' '+ spl_tokens['end'] + \"\\n\"\n",
    "        ipstr = ipstr.replace('  ', ' ')\n",
    "        outs.append(ipstr)\n",
    "        if (count % 10000 == 0):\n",
    "            print(count)\n",
    "\n",
    "    out_file = open(output_file_path, 'w', encoding='utf-8')\n",
    "    for i in outs:\n",
    "        out_file.write(i)\n",
    "    out_file.close()\n",
    "\n",
    "    repl_file = open(attr_replacements_file, 'w', encoding='utf-8')\n",
    "    for i in repls:\n",
    "        repl_file.write(i)\n",
    "    repl_file.close()\n",
    "    \n",
    "    return c_noisy, count, c_empty_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "17802 266041 87371 9252 177218 84265\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 0.1\n",
    "c_noisy_pos, count_pos, count_empty_arr_pos = prepare_train_data_file(conts_pos, attrs_pos, data_pos, POS_TRAIN_OUT_FILE_PATH, POS_REPLACE_ATTRS_PATH, SAMPLE_RATE)\n",
    "c_noisy_neg, count_neg, count_empty_arr_neg = prepare_train_data_file(conts_neg, attrs_neg, data_neg, NEG_TRAIN_OUT_FILE_PATH, NEG_REPLACE_ATTRS_PATH, SAMPLE_RATE)\n",
    "print(c_noisy_pos, count_pos, count_empty_arr_pos, c_noisy_neg, count_neg, count_empty_arr_neg)\n",
    "\n",
    "#YELP:\n",
    "#17802 266041 87371 9252 177218 84265\n",
    "\n",
    "#AMAZON:\n",
    "#(9819, 277228, 172062, 10760, 277769, 142807)\n",
    "#(9859, 277228, 172062, 10604, 277769, 142807)\n",
    "# empty percentages - pos and neg - (0.6206515936341207, 0.5141214462377011)\n",
    "# percentage samples noised by first sampling and then edit distance - pos and neg - (0.035418500295785416, 0.03873722409628144)\n",
    "# percentage samples noised by first sampling and then edit distance - pos and neg - (0.03556278586578556, 0.03817560634916063)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_file(reference_file_path, output_file_path, attrs_path = None):\n",
    "    train_attrs = []\n",
    "    count = 0\n",
    "    outs = []\n",
    "    with open(reference_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            line = line.strip()\n",
    "            tokens = line.split(' ')\n",
    "            cont, attr = extract_attribute(attribute_vocab, tokens)\n",
    "            ipstr = spl_tokens['attr_start'] + ' ' + ' '.join(attr)  + ' ' + spl_tokens['cont_start'] + ' ' + ' '.join(cont)  + ' ' + spl_tokens[\n",
    "                'data_start'] + ' ' + line + ' '+ spl_tokens['end'] + \"\\n\"\n",
    "            ipstr = ipstr.replace('  ', ' ')\n",
    "            outs.append(ipstr)\n",
    "            if attrs_path:\n",
    "                train_attrs.append(' '.join(attr)+\"\\n\")\n",
    "            if (count % 10000 == 0):\n",
    "                print(count)\n",
    "    out_file = open(output_file_path, 'w', encoding='utf-8')\n",
    "    for i in outs:\n",
    "        out_file.write(i)\n",
    "    out_file.close()\n",
    "    if attrs_path:\n",
    "        out_file = open(conts_path, 'w', encoding='utf-8')\n",
    "        for i in train_conts:\n",
    "            out_file.write(i)\n",
    "        out_file.close()\n",
    "prepare_data_file(POS_VAL_FILE_PATH, POS_VAL_OUT_FILE_PATH)\n",
    "prepare_data_file(NEG_VAL_FILE_PATH, NEG_VAL_OUT_FILE_PATH)\n",
    "prepare_data_file(POS_TEST_FILE_PATH, POS_TEST_OUT_FILE_PATH)\n",
    "prepare_data_file(NEG_TEST_FILE_PATH, NEG_TEST_OUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_targets(file_path, out_file_with_input_only_path):\n",
    "    f = open(file_path, 'r')\n",
    "    test_lines = f.readlines()\n",
    "    f.close()\n",
    "    test_inputs = [re.sub(r'<DATA_START>.+<END>\\n', '', s)+'<DATA_START>' for s in test_lines]\n",
    "    fout = open(out_file_with_input_only_path, 'w')\n",
    "    fout.write('\\n'.join(test_inputs))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_targets(POS_TEST_OUT_FILE_PATH, POS_TEST_OUT_FILE_INPUTS_PATH)\n",
    "remove_targets(NEG_TEST_OUT_FILE_PATH, NEG_TEST_OUT_FILE_INPUTS_PATH)\n",
    "remove_targets(FROM_POS_REF_OUT_FILE_PATH, FROM_POS_REF_OUT_FILE_INPUTS_PATH)\n",
    "remove_targets(FROM_NEG_REF_OUT_FILE_PATH, FROM_NEG_REF_OUT_FILE_INPUTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_shuffle(pos_file_path, neg_file_path, combined_file_path):\n",
    "    fp = open(pos_file_path, 'r', encoding='utf-8')\n",
    "    fn = open(neg_file_path, 'r', encoding='utf-8')\n",
    "    out = fp.readlines() + fn.readlines()\n",
    "    shuffle(out)\n",
    "    fo = open(combined_file_path, 'w', encoding='utf-8')\n",
    "    for i in out:\n",
    "        fo.write(i)\n",
    "    fo.close()\n",
    "combine_and_shuffle(POS_TRAIN_OUT_FILE_PATH, NEG_TRAIN_OUT_FILE_PATH, COMBINED_TRAIN_OUT_FILE_PATH)\n",
    "combine_and_shuffle(POS_VAL_OUT_FILE_PATH, NEG_VAL_OUT_FILE_PATH, COMBINED_VAL_OUT_FILE_PATH)\n",
    "combine_and_shuffle(POS_TEST_OUT_FILE_PATH, NEG_TEST_OUT_FILE_PATH, COMBINED_TEST_OUT_FILE_PATH)\n",
    "combine_and_shuffle(POS_TEST_OUT_FILE_INPUTS_PATH, NEG_TEST_OUT_FILE_INPUTS_PATH, COMBINED_TEST_OUT_FILE_INPUTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORMANCE CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "OUT_DATA = '/home/ubuntu/git-repos/pytorch-pretrained-BERT/runs/amazon_1_plus_3_epoch'\n",
    "# OUT_DATA = '/home/ubuntu/git-repos/pytorch-pretrained-BERT/runs/yelp_3_epoch'\n",
    "POS_TEST_FILE_OUT_PATH = os.path.join(OUT_DATA, 'sentiment.test.out.1')\n",
    "NEG_TEST_FILE_OUT_PATH = os.path.join(OUT_DATA, 'sentiment.test.out.0')\n",
    "TO_NEG_REF_FILE_OUT_PATH = os.path.join(OUT_DATA, 'sentiment.reference.out.from.1')\n",
    "TO_POS_REF_FILE_OUT_PATH = os.path.join(OUT_DATA, 'sentiment.reference.out.from.0')\n",
    "NO_COMMON_WORDS_TO_NEG_REF = os.path.join(OUT_DATA, 'sentiment.reference.no_common.from.1')\n",
    "NO_COMMON_WORDS_TO_POS_REF = os.path.join(OUT_DATA, 'sentiment.reference.no_common.from.0')\n",
    "NO_COMMON_WORDS_POS_TEST = os.path.join(OUT_DATA, 'sentiment.test.no_common.1')\n",
    "NO_COMMON_WORDS_NEG_TEST = os.path.join(OUT_DATA, 'sentiment.test.no_common.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from : /home/ubuntu/git-repos/pytorch-pretrained-BERT/runs/amazon_1_plus_3_epoch and /home/ubuntu/data/amazon\n",
      "Positive test pred - src: 92.3\n",
      "Negative test pred - src: 91.21\n",
      "Positive to Negative reference pred - tgt: 38.55\n",
      "Negative to Positive reference pred - tgt: 35.13\n",
      "Positive to Negative reference tgt - src: 47.6\n",
      "Negative to Positive reference tgt - src: 43.78\n",
      "Positive to Negative reference pred - src: 80.84\n",
      "Negative to Positive reference pred - src: 81.28\n"
     ]
    }
   ],
   "source": [
    "def get_BLEU(preds_file, targets_file):\n",
    "    hypotheses = []\n",
    "    reference = []\n",
    "    with open(preds_file) as fp1: # Rename file path with test results\n",
    "        hypotheses = fp1.readlines()\n",
    "    with open(targets_file) as fp1: # Path of the reference file\n",
    "        reference = fp1.readlines()\n",
    "\n",
    "    reference = list(map(lambda x: x.strip(), reference))\n",
    "    hypotheses=list(map(lambda x: x.strip().replace('<END>',''), hypotheses))\n",
    "    return get_moses_multi_bleu(hypotheses, reference, lowercase=True)\n",
    "print('Data from : {} and {}'.format(OUT_DATA,DATA))\n",
    "print(\"Positive test pred - src:\", get_BLEU(POS_TEST_FILE_OUT_PATH, POS_TEST_FILE_PATH))\n",
    "print(\"Negative test pred - src:\", get_BLEU(NEG_TEST_FILE_OUT_PATH, NEG_TEST_FILE_PATH))\n",
    "print(\"Positive to Negative reference pred - tgt:\", get_BLEU(TO_NEG_REF_FILE_OUT_PATH, TO_NEG_REF_FILE_PATH))\n",
    "print(\"Negative to Positive reference pred - tgt:\", get_BLEU(TO_POS_REF_FILE_OUT_PATH, TO_POS_REF_FILE_PATH))\n",
    "print(\"Positive to Negative reference tgt - src:\", get_BLEU(TO_NEG_REF_FILE_PATH, FROM_POS_REF_FILE_PATH))\n",
    "print(\"Negative to Positive reference tgt - src:\", get_BLEU(TO_POS_REF_FILE_PATH, FROM_NEG_REF_FILE_PATH))\n",
    "print(\"Positive to Negative reference pred - src:\", get_BLEU(TO_NEG_REF_FILE_OUT_PATH, FROM_POS_REF_FILE_PATH))\n",
    "print(\"Negative to Positive reference pred - src:\", get_BLEU(TO_POS_REF_FILE_OUT_PATH, FROM_NEG_REF_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_parts(source_file, pred_file, target_file=None, out_file=None):\n",
    "    srcs = open(source_file).readlines()\n",
    "    preds = open(pred_file).readlines()\n",
    "    tgts = None\n",
    "    if target_file:\n",
    "        tgts = open(target_file).readlines()\n",
    "    srcs_out = []\n",
    "    preds_out = []\n",
    "    tgts_out = []\n",
    "    for i in range(len(srcs)):\n",
    "        common = set(srcs[i].split()) & set(preds[i].split())\n",
    "        if target_file:\n",
    "            common = common & set(tgts[i].split())\n",
    "        srcs_out.append(' '.join([wrd for wrd in srcs[i].split() if wrd not in common]))\n",
    "        preds_out.append(' '.join([wrd for wrd in preds[i].split() if wrd not in common]))\n",
    "        if target_file:\n",
    "            tgts_out.append(' '.join([wrd for wrd in tgts[i].split() if wrd not in common]))\n",
    "    f = open(out_file, 'w')\n",
    "    for i in range(len(srcs)):\n",
    "        f.write('SRC : '+srcs[i])\n",
    "        f.write('PRED: '+preds[i])\n",
    "        if target_file:\n",
    "            f.write('TGT : '+tgts[i])\n",
    "        f.write('--------------\\n')\n",
    "        f.write('SRC_UNIQUE : '+srcs_out[i]+'\\n')\n",
    "        f.write('PRED_UNIQUE: '+preds_out[i]+'\\n')\n",
    "        if target_file:\n",
    "            f.write('TGT_UNIQUE : '+tgts_out[i]+'\\n')\n",
    "        f.write('==============\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_common_parts(POS_TEST_FILE_PATH, POS_TEST_FILE_OUT_PATH, None, NO_COMMON_WORDS_POS_TEST)\n",
    "remove_common_parts(NEG_TEST_FILE_PATH, NEG_TEST_FILE_OUT_PATH, None, NO_COMMON_WORDS_NEG_TEST)\n",
    "remove_common_parts(FROM_POS_REF_FILE_PATH, TO_NEG_REF_FILE_OUT_PATH, TO_NEG_REF_FILE_PATH, NO_COMMON_WORDS_TO_NEG_REF)\n",
    "remove_common_parts(FROM_NEG_REF_FILE_PATH, TO_POS_REF_FILE_OUT_PATH, TO_POS_REF_FILE_PATH, NO_COMMON_WORDS_TO_POS_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREP FOR TRAINING DEEPMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = '/home/ubuntu/data/amazon'\n",
    "DATA = '/home/ubuntu/data/yelp'\n",
    "POS_TRAIN_FILE_PATH = os.path.join(DATA, 'sentiment.train.1')\n",
    "NEG_TRAIN_FILE_PATH = os.path.join(DATA,'sentiment.train.0')\n",
    "POS_VAL_FILE_PATH = os.path.join(DATA, 'sentiment.dev.1')\n",
    "NEG_VAL_FILE_PATH = os.path.join(DATA,'sentiment.dev.0')\n",
    "POS_TEST_FILE_PATH = os.path.join(DATA, 'sentiment.test.1')\n",
    "NEG_TEST_FILE_PATH = os.path.join(DATA,'sentiment.test.0')\n",
    "data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = open(POS_TRAIN_FILE_PATH, 'r').readlines()\n",
    "neg_data = open(NEG_TRAIN_FILE_PATH, 'r').readlines()\n",
    "pos_val_data = open(POS_VAL_FILE_PATH, 'r').readlines()\n",
    "neg_val_data = open(NEG_VAL_FILE_PATH, 'r').readlines()\n",
    "pos_test_data = open(POS_TEST_FILE_PATH, 'r').readlines()\n",
    "neg_test_data = open(NEG_TEST_FILE_PATH, 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [(i.strip(), {'label':1}) for i in pos_data] \n",
    "neg = [(i.strip(), {'label':0}) for i in neg_data]\n",
    "pos_val = [(i.strip(), {'label':1}) for i in pos_val_data] \n",
    "neg_val = [(i.strip(), {'label':0}) for i in neg_val_data]\n",
    "pos_test= [(i.strip(), {'label':1}) for i in pos_test_data] \n",
    "neg_test = [(i.strip(), {'label':0}) for i in neg_test_data]\n",
    "pos_neg = pos+neg\n",
    "pos_neg_val = pos_val+neg_val\n",
    "pos_neg_test = pos_test+neg_test\n",
    "random.shuffle(pos_neg)\n",
    "random.shuffle(pos_neg_val)\n",
    "random.shuffle(pos_neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[str('texts')] = [i[0] for i in pos_neg]+[i[0] for i in pos_neg_val]+[i[0] for i in pos_neg_test]\n",
    "data[str('info')] = [i[1] for i in pos_neg]+[i[1] for i in pos_neg_val]+[i[1] for i in pos_neg_test]\n",
    "data[str('train_ind')] = list(range(len(pos_neg)))\n",
    "data[str('val_ind')] = list(range(len(pos_neg),len(pos_neg)+len(pos_neg_val)))\n",
    "data[str('test_ind')] = list(range(len(pos_neg)+len(pos_neg_val),len(pos_neg)+len(pos_neg_val)+len(pos_neg_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict(data), open('/home/ubuntu/raw.pickle', 'wb'), protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
